{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import load_model, Visualizer\n",
    "import numpy as np\n",
    "from utils import DataHandler, Visualizer\n",
    "from model import Model\n",
    "from layers import FullyConnectedLayer, ActivationReLU, ActivationSoftmax\n",
    "from training_core import SGDOptimizer, CELoss, Trainer\n",
    "from evaluate import Evaluator\n",
    "from metrics import Accuracy\n",
    "\n",
    "np.random.seed(0)\n",
    "data_handler = DataHandler('.\\data')\n",
    "X_train, y_train, X_test, y_test = data_handler.load_mnist()\n",
    "X, y, X_val, y_val = data_handler.split_validation(\n",
    "    X_train, y_train, val_ratio=0.25)\n",
    "X, X_test = data_handler.scale(X, X_test)\n",
    "X_val = data_handler.scale(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_neuron_layer=128, learning_rate=1, \n",
    "                 decay=0.1, moment=0.8, batch_size=128, \n",
    "                 l2_reg_weight=0, n_epoch=5):\n",
    "    # 3-layer neural network\n",
    "    model = Model()\n",
    "    model.add(FullyConnectedLayer(X.shape[1], n_neuron_layer, l2_reg_weight))\n",
    "    model.add(ActivationReLU())\n",
    "    model.add(FullyConnectedLayer(\n",
    "        n_neuron_layer, n_neuron_layer, l2_reg_weight))\n",
    "    model.add(ActivationReLU())\n",
    "    model.add(FullyConnectedLayer(n_neuron_layer, 10, l2_reg_weight))\n",
    "    model.add(ActivationSoftmax())\n",
    "    model.set_items(loss=CELoss(), optimizer=SGDOptimizer(learning_rate, decay, moment), accuracy=Accuracy())\n",
    "    model.finalize()\n",
    "    \n",
    "    trainer = Trainer(model)\n",
    "    trainer.train(X, y, epochs=n_epoch, batch_size=batch_size,\n",
    "                print_every=None, val_data=(X_val, y_val))\n",
    "    model.set_parameters(model.best_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----n_neuron_layer=32----\n",
      "Training--acc: 0.8433, loss: 0.3620 (data_loss: 0.3620, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8359, loss: 0.4574\n",
      "### Training finished!\n",
      "\n",
      "----n_neuron_layer=64----\n",
      "Training--acc: 0.8638, loss: 0.2639 (data_loss: 0.2639, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8531, loss: 0.3947\n",
      "### Training finished!\n",
      "\n",
      "----n_neuron_layer=128----\n",
      "Training--acc: 0.8613, loss: 0.2914 (data_loss: 0.2914, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8543, loss: 0.3983\n",
      "### Training finished!\n",
      "\n",
      "----n_neuron_layer=256----\n",
      "Training--acc: 0.8711, loss: 0.2524 (data_loss: 0.2524, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8633, loss: 0.3763\n",
      "### Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Number of neurons in the hidden layer\n",
    "n_neuron_layer = [32, 64, 128, 256]\n",
    "for n in n_neuron_layer:\n",
    "    print(f\"\\n----n_neuron_layer={n}----\")\n",
    "    model = create_model(n_neuron_layer=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very closed, though the performance improves slightly as the number of hidden layer neurons increases. To save computing time, we select `n_neuron_layer=128` as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----batch_size=32----\n",
      "Training--acc: 0.8392, loss: 0.1392 (data_loss: 0.1392, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8298, loss: 0.4757\n",
      "### Training finished!\n",
      "\n",
      "----batch_size=64----\n",
      "Training--acc: 0.8594, loss: 0.1481 (data_loss: 0.1481, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8476, loss: 0.4185\n",
      "### Training finished!\n",
      "\n",
      "----batch_size=128----\n",
      "Training--acc: 0.8728, loss: 0.3470 (data_loss: 0.3470, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8605, loss: 0.3945\n",
      "### Training finished!\n",
      "\n",
      "----batch_size=256----\n",
      "Training--acc: 0.8604, loss: 0.4013 (data_loss: 0.4013, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8511, loss: 0.4206\n",
      "### Training finished!\n"
     ]
    }
   ],
   "source": [
    "batch_size = [32, 64, 128, 256]\n",
    "for bs in batch_size:\n",
    "    print(f\"\\n----batch_size={bs}----\")\n",
    "    model = create_model(batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the performance of the models peaks at n=128, and as `batch_size` increases, the training time gets shorter. We then select 128 as the default `batch_size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----learning_rate=1----\n",
      "Training--acc: 0.8669, loss: 0.3720 (data_loss: 0.3720, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8550, loss: 0.4037\n",
      "### Training finished!\n",
      "\n",
      "----learning_rate=0.1----\n",
      "Training--acc: 0.7382, loss: 0.7104 (data_loss: 0.7104, reg_loss: 0.0000)\n",
      "Validation--acc: 0.7297, loss: 0.7365\n",
      "### Training finished!\n",
      "\n",
      "----learning_rate=0.01----\n",
      "Training--acc: 0.3973, loss: 2.2986 (data_loss: 2.2986, reg_loss: 0.0000)\n",
      "Validation--acc: 0.3947, loss: 2.2982\n",
      "### Training finished!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [1, 0.1, 0.01]\n",
    "for lr in learning_rate:\n",
    "    print(f\"\\n----learning_rate={lr}----\")\n",
    "    model = create_model(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, a lower initial `learning_rate` leads to a poorer performance. This could be due to the fact that the learning rate is too low to allow the model to learn the data. We then select 1 as the default `learning_rate` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----decay=1----\n",
      "Training--acc: 0.7659, loss: 0.6317 (data_loss: 0.6317, reg_loss: 0.0000)\n",
      "Validation--acc: 0.7580, loss: 0.6637\n",
      "### Training finished!\n",
      "\n",
      "----decay=0.1----\n",
      "Training--acc: 0.8682, loss: 0.3474 (data_loss: 0.3474, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8561, loss: 0.4074\n",
      "### Training finished!\n",
      "\n",
      "----decay=0.01----\n",
      "Training--acc: 0.7846, loss: 0.6432 (data_loss: 0.6432, reg_loss: 0.0000)\n",
      "Validation--acc: 0.7640, loss: 0.6848\n",
      "### Training finished!\n",
      "\n",
      "----decay=0.001----\n",
      "Training--acc: 0.1105, loss: 2.3131 (data_loss: 2.3131, reg_loss: 0.0000)\n",
      "Validation--acc: 0.0987, loss: 2.3042\n",
      "### Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Learning rate decay factor\n",
    "decay = [1, 0.1, 0.01, 0.001]\n",
    "for d in decay:\n",
    "    print(f\"\\n----decay={d}----\")\n",
    "    model = create_model(decay=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the performance of the models peaks at `decay=0.1`, and as `decay` gets smaller, the metrics get worse quickly. Thus, we cannot let lr decay too slowly. We then select 0.1 as the default `decay` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----moment=0.0----\n",
      "Training--acc: 0.8159, loss: 0.4779 (data_loss: 0.4779, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8039, loss: 0.5426\n",
      "### Training finished!\n",
      "\n",
      "----moment=0.7----\n",
      "Training--acc: 0.8652, loss: 0.3521 (data_loss: 0.3521, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8569, loss: 0.4027\n",
      "### Training finished!\n",
      "\n",
      "----moment=0.8----\n",
      "Training--acc: 0.8709, loss: 0.3646 (data_loss: 0.3646, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8609, loss: 0.3933\n",
      "### Training finished!\n",
      "\n",
      "----moment=0.9----\n",
      "Training--acc: 0.8449, loss: 0.4187 (data_loss: 0.4187, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8381, loss: 0.4620\n",
      "### Training finished!\n"
     ]
    }
   ],
   "source": [
    "moment = [0.0, 0.7, 0.8, 0.9]\n",
    "for m in moment:\n",
    "    print(f\"\\n----moment={m}----\")\n",
    "    model = create_model(moment=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum parameter indeed improves the performance. But we do not find a clear difference between 0.7 and 0.8, though a too high momentum may lead to overshooting. We select 0.8 as the default `momentum` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----l2_reg_weight=0----\n",
      "Training--acc: 0.8689, loss: 0.3388 (data_loss: 0.3388, reg_loss: 0.0000)\n",
      "Validation--acc: 0.8539, loss: 0.4009\n",
      "### Training finished!\n",
      "\n",
      "----l2_reg_weight=0.1----\n",
      "Training--acc: 0.0982, loss: 2.3028 (data_loss: 2.3028, reg_loss: 0.0000)\n",
      "Validation--acc: 0.0969, loss: 2.3028\n",
      "### Training finished!\n",
      "\n",
      "----l2_reg_weight=0.01----\n",
      "Training--acc: 0.8364, loss: 0.7465 (data_loss: 0.4995, reg_loss: 0.2470)\n",
      "Validation--acc: 0.8305, loss: 0.4957\n",
      "### Training finished!\n",
      "\n",
      "----l2_reg_weight=0.001----\n",
      "Training--acc: 0.8703, loss: 0.4287 (data_loss: 0.3301, reg_loss: 0.0987)\n",
      "Validation--acc: 0.8579, loss: 0.3966\n",
      "### Training finished!\n",
      "\n",
      "----l2_reg_weight=0.0001----\n",
      "Training--acc: 0.8654, loss: 0.3941 (data_loss: 0.3737, reg_loss: 0.0204)\n",
      "Validation--acc: 0.8551, loss: 0.4080\n",
      "### Training finished!\n"
     ]
    }
   ],
   "source": [
    "l2_reg_weight = [0, 0.1, 0.01, 0.001, 0.0001]\n",
    "for l2 in l2_reg_weight:\n",
    "    print(f\"\\n----l2_reg_weight={l2}----\")\n",
    "    model = create_model(l2_reg_weight=l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above and previous results, the training and validation losses and accuracies are not far from each other, which indicates that the model is not overfitting. We can then select `l2_reg_weight=0` as the default parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./result/fashion_mnist_model.pkl')\n",
    "\n",
    "# Visualize the weights and biases of the first layer\n",
    "Visualizer.visualize_weights(model.best_weights[0][0])\n",
    "Visualizer.visualize_biases(model.best_weights[0][1].reshape(-1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
